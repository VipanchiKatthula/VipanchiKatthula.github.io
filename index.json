[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a passionate Data Scientist with 3+ years of exprience, currently pursuing Masters in Business Analytics at University of Illinois at Chicago with specialization in Data Science. I\u0026rsquo;m working with Prof Ranganathan Chandrasekaran as an Machine Learning researcher and co-authored \u0026ldquo;Use of Wearable Healthcare Devices by US adults: Patterns of Use and Key Predictors\u0026rdquo; [\rLink].\nI am excited to combine my experience and knowledge from Grad school in dealing with real life problems. I have written multiple blog posts on Towards Data Science and on this website on various machine learning concepts to extend my contribution to the data science community.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vipanchikatthula.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a passionate Data Scientist with 3+ years of exprience, currently pursuing Masters in Business Analytics at University of Illinois at Chicago with specialization in Data Science. I\u0026rsquo;m working with Prof Ranganathan Chandrasekaran as an Machine Learning researcher and co-authored \u0026ldquo;Use of Wearable Healthcare Devices by US adults: Patterns of Use and Key Predictors\u0026rdquo; [\rLink].","tags":null,"title":"Vipanchi Reddy Katthula","type":"authors"},{"authors":null,"categories":null,"content":"Finding Similarity between documents using Jaccard Similarity and including POS Tags of the words in the model to better find the synchronous relationship between documents.\nAdd anything you want here\n","date":1583625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583625600,"objectID":"e789299400d86e43c975e1270e3b32bf","permalink":"https://vipanchikatthula.github.io/project/document-similarity-with-postags/","publishdate":"2020-03-08T00:00:00Z","relpermalink":"/project/document-similarity-with-postags/","section":"project","summary":"Finding the closeness between documents.","tags":["NLP"],"title":"Document Similarity using POS Tags","type":"project"},{"authors":null,"categories":null,"content":"PySpark Operations On Amazon Tweets This repository shows the implementation of Spark context, Spark SQL context on Amazon Tweets data set with 400k Tweets. I dealt with tweet_id (id_str), Tweet_created_time, Retweet_count, Favourite_count to find the days with high influx of tweets.\nAnalyzed the tweets on the busiest day to find the words that were repeated the most in the selected tweets.\nSummary of operations:\n1. Import csv file:\ndata = spark.read.format(\u0026ldquo;csv\u0026rdquo;).option(\u0026ldquo;header\u0026rdquo;,\u0026ldquo;true\u0026rdquo;).load(\u0026ldquo;filepath/filename.csv\u0026rdquo;)\n2. Selecting the required columns from a data frame:\ndata.select(\u0026ldquo;column_name1\u0026rdquo;,\u0026lsquo;colname2\u0026rsquo;,\u0026lsquo;colname3\u0026rsquo;,\u0026lsquo;colname4\u0026rsquo;)\n3. Printing data types of columns:\ntypes = [f.dataType for f in data1.schema.fields]\ntypes\n4. Printing distinct values of a column:\ndata.select(\u0026ldquo;column_name\u0026rdquo;).distinct().show()\n5. Parsing a column to create additional columns:\nfrom pyspark.sql.functions import split\ntweet_created_at is in the format: \u0026ldquo;Tue Nov 01 02:39:55 +0000 2016\u0026rdquo;\na = split(dat_filtered[\u0026ldquo;tweet_created_at\u0026rdquo;], ' \u0026lsquo;)\ndat_filtered = dat_filtered.withColumn(\u0026lsquo;Month\u0026rsquo;, a.getItem(1))\ndat_filtered = dat_filtered1.withColumn(\u0026lsquo;Date\u0026rsquo;, a.getItem(2))\ndat_filtered = dat_filtered1.withColumn(\u0026lsquo;Year\u0026rsquo;, a.getItem(5))\n6. Concatenating multiple columns to form a new one\ndat_filtered.select(concat(col(\u0026ldquo;Month\u0026rdquo;), lit(\u0026rdquo; \u0026ldquo;), col(\u0026ldquo;Date\u0026rdquo;),lit(\u0026rdquo; \u0026ldquo;), col(\u0026ldquo;Year\u0026rdquo;)).alias(\u0026ldquo;Date\u0026rdquo;))\n7. Importing SQL funtions col,lit\nimport pyspark.sql.functions as sq dat_filtered.withColumn(\u0026ldquo;tweet_created_at\u0026rdquo;,sq.concat(col(\u0026ldquo;Month\u0026rdquo;), sq.lit(\u0026rdquo; \u0026ldquo;), sq.col(\u0026ldquo;Date\u0026rdquo;),sq.lit(\u0026rdquo; \u0026ldquo;), sq.col(\u0026ldquo;Year\u0026rdquo;)))\n8. Aggregations on dataframe\ndf.groupby(df.date).agg(sq.count(\u0026lsquo;id_str\u0026rsquo;).alias(\u0026ldquo;count_of_tweets\u0026rdquo;))\nAbove command counts the number of tweets grouped by the date\n9. Initializing spark context and sql context to perform SQL queries\nconf = pyspark.SparkConf()\nsc = pyspark.SparkContext.getOrCreate(conf=conf)\nfrom pyspark.sql import SQLContext\nsqlcontext = SQLContext(sc)\ncounts.registerTempTable(\u0026ldquo;tmpcounts\u0026rdquo;)\ncounts_ordered = sqlcontext.sql(\u0026ldquo;SELECT * FROM tmpcounts order by count_of_tweets desc limit 5\u0026rdquo;)\n","date":1582416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582416000,"objectID":"e1302cd4aee18b2fc4351e5b6f69332d","permalink":"https://vipanchikatthula.github.io/project/pyspark-operations-on-amazons-tweets/","publishdate":"2020-02-23T00:00:00Z","relpermalink":"/project/pyspark-operations-on-amazons-tweets/","section":"project","summary":"Finding the word frequencies and basic data analysis using PySpark.","tags":["Big Data","NLP"],"title":"400K Tweet analysis on PySpark","type":"project"},{"authors":null,"categories":null,"content":"","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"956fbdc3af54673dad8a6dfc1e82892a","permalink":"https://vipanchikatthula.github.io/project/twitter-sentiment-analysis/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/project/twitter-sentiment-analysis/","section":"project","summary":"Finding the binary sentiment of the tweets and basics of text processing.","tags":["NLP"],"title":"Sentiment Analysis on Tweets","type":"project"},{"authors":null,"categories":null,"content":"I created a Mapper-Reducer implementation using basic features of Python to mimic Hadoop\u0026rsquo;s mapper-reducer functionality. I used Multi-threading (explained later) to run parallel operations. I used Google Colab to run the model and you can find the required [\u0026ldquo;Pride_and_Prejudice.txt\u0026rdquo;] file here.\nGoal First, let\u0026rsquo;s map the words in the provided text to 1 using the mapper as \u0026lt;Word,1\u0026gt; and then use reducer to find the word count in the format \u0026lt;Word,Count\u0026gt;. A reducer also sorts the words which needs to be separately added to the series of operations. I implement the multi threading in python to parallelly get the word counts from two lists of words.\nInput File inputfile = open('/content/drive/My Drive/IDS561BigData/Assignment1/Pride_and_Prejudice.txt',\u0026quot;r\u0026quot;)\rtext = inputfile.read()\r Data cleaning In order for us to use the text from the book, we have to remove the the punctuations, unnecessary numbers as they won\u0026rsquo;t help us understand the text in most cases.\nWe remove the numbers, convert the text to lower case using .lower() command and use Regular Expressions to replace punctuations with a space ' '. I then remove those Lines with no text in them.\ndef data_clean(text):\rNoNumbers = ''.join([i for i in text if not i.isdigit()]) #Removing numbers\rNoNumbers = text.lower() #Making the text to lower case\rimport re\ronlyText = re.sub(r\u0026quot;[^a-z\\s]+\u0026quot;,' ',NoNumbers) #Removing punctuation\rfinaltext = \u0026quot;\u0026quot;.join([s for s in onlyText.strip().splitlines(True) if s.strip()]) #Removing the null lines\rreturn finaltext\r Splitting data into two parts Let\u0026rsquo;s define a reusable function which takes a list of words as input to do multi-threading on the given data set. Here \u0026ldquo;a\u0026rdquo; is the number of words after you wish to make the split. For example, splitlines(text,200) will split the text into split1 and split2 as two sentences with first 1 to 199 words in split1 and rest in split2.\ndef splitlines(text,a):\rlinessplit = text.splitlines() #Splitting the lines into a list\rsplit1 = linessplit[0:a] #Creating the first split with the first \u0026quot;a\u0026quot; number of lines into split 1\rsplit2 = linessplit[a:] #Creating the second split with the first \u0026quot;a\u0026quot; number of lines into split 2\rreturn split1,split2\r Mapper We map all the words in \u0026ldquo;text\u0026rdquo; to 1 using the keyval.append([j,1]) command. So, the key here is the word and we apped a value of 1. The output format of the data is \u0026lt;word,1\u0026gt;.\ndef mapper(text,out_queue):\rkeyval = []\rfor i in text:\rwordssplit = i.split()\rfor j in wordssplit:\rkeyval.append([j,1]) #Appending each word in the line with 1 and storing it in [\u0026quot;word\u0026quot;,1] format in a nested list\rout_queue.put(keyval)\r Sorting Function As we have two lists of separate key-value pairs after the split function, now we define sortging function to handle both the lists. We take two inputs and return only one output which which contains the sorted list of word key value pairs.\ndef sortedlists(list1,list2):\rout = list1 + list2 #Appending the two input lists into a single list\rout.sort(key= lambda x :x[0]) #Sorting the lists based on the first element of the list which is the \u0026quot;word\u0026quot;\rreturn out\r Partition The below code creates two lists of sorted words in alphabetical order and then we separate the words starting with a-m and n-z. The function returns the sorted lists which will be inputs to the reducer function.\ndef partition(sorted_list) :\rsort1out = []\rsort2out = []\rfor i in sorted_list:\rif i[0][0] \u0026lt; 'n': #Partitioning the sorted word list into two separate lists sort1out.append(i) #with first list containing words starting with a-m and n-z into second\relse : sort2out.append(i)\rreturn sort1out,sort2out\r Reducer def reducer(part_out1,out_queue) :\rsum_reduced = []\rcount = 1\rfor i in range(0,len(part_out1)):\rif i \u0026lt; len(part_out1)-1:\rif part_out1[i] == part_out1[i+1]:\rcount = count+1 #Counting the number of words\relse : sum_reduced.append([part_out1[i][0],count]) #Appending the word along with count to sum_reduced list as [\u0026quot;word\u0026quot;,count]\rcount = 1 else: sum_reduced.append(part_out1[i]) #Appending the last word to the output list out_queue.put(sum_reduced)\r Multi - Threading function The user defined function below takes a function and two inputs as arguments. The function is applied on both the inputs simultaneously and the output is returned by the function.\nimport threading\rimport queue\rdef multi_thread_function(func,map1_input,map2_input): #func is the function to be used with two threads taking two inputs map1_input and map2_input\rmy_queue1 = queue.Queue() #Using queue to store the values of mapper output to use them in sort function\rmy_queue2 = queue.Queue()\rt1 = threading.Thread(target=func, args=(map1_input,my_queue1)) t2 = threading.Thread(target=func, args=(map2_input,my_queue2)) t1.start() #Starting the execution of thread1\rt2.start() #Starting the execution of thread2 to run simultaneously with thread1\rt1.join() #Waiting for the thread1 to be completely executed\rt2.join() #Waiting for the thread2 to be completely executed\rlist1out = my_queue1.get() #Getting the values from the queue into a variable to return its value\rlist2out = my_queue2.get()\rreturn list1out,list2out\r Mapper-Reducer Finally, we combine all the above functions to split the lines after 5000 words and them implement the entire Mapper Reducer operation which mimics the way Hadoop\u0026rsquo;s handles the data.\ndef main_function(text): cleantext = data_clean(text)\rlinessplit = splitlines(cleantext,5000)\rmapperout = multi_thread_function(mapper,linessplit[0],linessplit[1]) sortedwords = sortedlists(mapperout[0],mapperout[1])\rslicedwords = partition(sortedwords)\rreducerout = multi_thread_function(reducer,slicedwords[0],slicedwords[1])\rreturn reducerout[0]+reducerout[1]\routput = main_function(text)\rimport pandas as pd\rpd.DataFrame(output).to_csv(\u0026quot;Output.csv\u0026quot;,index=False,header = [\u0026quot;Word\u0026quot;,\u0026quot;Frequency\u0026quot;]) #Saving file as a .csv file in the current directory\r Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"fbfe283bf92c6cca009dbe9a28d0b665","permalink":"https://vipanchikatthula.github.io/post/mapper-reducer-implementation/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/post/mapper-reducer-implementation/","section":"post","summary":"I created a Mapper-Reducer implementation using basic features of Python to mimic Hadoop\u0026rsquo;s mapper-reducer functionality. I used Multi-threading (explained later) to run parallel operations. I used Google Colab to run the model and you can find the required [\u0026ldquo;Pride_and_Prejudice.","tags":null,"title":"Map Reduce 101 - Python Implementation (Multi Threading)","type":"post"},{"authors":null,"categories":null,"content":"Text Analytics - Jaccard and Cosine Similarity Repository to showcase projects related to text analytics and Natural Language Processing (NLP)\nJaccard Similarity: The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. Itâ€™s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations.\n#How to calculate: The formula to find the Index is:\nJaccard Index = (the number in both sets) / (the number in either set) * 100\nThe same formula in notation is: J(X,Y) = |Xâˆ©Y| / |XâˆªY| In Steps, thatâ€™s:\n1.Count the number of members which are shared between both sets. 2.Count the total number of members in both sets (shared and un-shared). 3.Divide the number of shared members (1) by the total number of members (2). 4.Multiply the number you found in (3) by 100.\nCosine Similarity: Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. Any document can be represented by thousands of attributes, each recording the frequency of a particular word (such as a keyword) or phrase in the document. Thus, each document is an object represented by what is called a term-frequency vector.\nI used python function to calculate the text similarity rather than using the traditional way of calculating by using the formula.\nSources: https://www.statisticshowto.datasciencecentral.com/jaccard-index/\nhttps://www.sciencedirect.com/topics/computer-science/cosine-similarity\n","date":1580601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580601600,"objectID":"3d0710ee4f5d5708e09c3d6caba4847e","permalink":"https://vipanchikatthula.github.io/project/jaccard-cosine-similarity/","publishdate":"2020-02-02T00:00:00Z","relpermalink":"/project/jaccard-cosine-similarity/","section":"project","summary":"Finding similarity between documents.","tags":["NLP"],"title":"Jaccard Cosine Similarity","type":"project"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python\rimport pandas as pd\rdata = pd.read_csv(\u0026quot;data.csv\u0026quot;)\rdata.head()\r```\r renders as\nimport pandas as pd\rdata = pd.read_csv(\u0026quot;data.csv\u0026quot;)\rdata.head()\r Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}\r{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$\r renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\r1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\r renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid\rgraph TD\rA[Hard] --\u0026gt;|Text| B(Round)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result 1]\rC --\u0026gt;|Two| E[Result 2]\r```\r renders as\ngraph TD\rA[Hard] --\u0026gt;|Text| B(Round)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result 1]\rC --\u0026gt;|Two| E[Result 2]\r An example sequence diagram:\n```mermaid\rsequenceDiagram\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts!\rJohn--\u0026gt;\u0026gt;Alice: Great!\rJohn-\u0026gt;\u0026gt;Bob: How about you?\rBob--\u0026gt;\u0026gt;John: Jolly good!\r```\r renders as\nsequenceDiagram\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts!\rJohn--\u0026gt;\u0026gt;Alice: Great!\rJohn-\u0026gt;\u0026gt;Bob: How about you?\rBob--\u0026gt;\u0026gt;John: Jolly good!\r An example Gantt diagram:\n```mermaid\rgantt\rsection Section\rCompleted :done, des1, 2014-01-06,2014-01-08\rActive :active, des2, 2014-01-07, 3d\rParallel 1 : des3, after des1, 1d\rParallel 2 : des4, after des1, 1d\rParallel 3 : des5, after des3, 1d\rParallel 4 : des6, after des4, 1d\r```\r renders as\ngantt\rsection Section\rCompleted :done, des1, 2014-01-06,2014-01-08\rActive :active, des2, 2014-01-07, 3d\rParallel 1 : des3, after des1, 1d\rParallel 2 : des4, after des1, 1d\rParallel 3 : des5, after des3, 1d\rParallel 4 : des6, after des4, 1d\r An example class diagram:\n```mermaid\rclassDiagram\rClass01 \u0026lt;|-- AveryLongClass : Cool\r\u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01\rClass09 --\u0026gt; C2 : Where am i?\rClass09 --* C3\rClass09 --|\u0026gt; Class07\rClass07 : equals()\rClass07 : Object[] elementData\rClass01 : size()\rClass01 : int chimp\rClass01 : int gorilla\rclass Class10 {\r\u0026lt;\u0026lt;service\u0026gt;\u0026gt;\rint id\rsize()\r}\r```\r renders as\nclassDiagram\rClass01 \u0026lt;|-- AveryLongClass : Cool\r\u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01\rClass09 --\u0026gt; C2 : Where am i?\rClass09 --* C3\rClass09 --|\u0026gt; Class07\rClass07 : equals()\rClass07 : Object[] elementData\rClass01 : size()\rClass01 : int chimp\rClass01 : int gorilla\rclass Class10 {\r\u0026lt;\u0026lt;service\u0026gt;\u0026gt;\rint id\rsize()\r}\r An example state diagram:\n```mermaid\rstateDiagram\r[*] --\u0026gt; Still\rStill --\u0026gt; [*]\rStill --\u0026gt; Moving\rMoving --\u0026gt; Still\rMoving --\u0026gt; Crash\rCrash --\u0026gt; [*]\r```\r renders as\nstateDiagram\r[*] --\u0026gt; Still\rStill --\u0026gt; [*]\rStill --\u0026gt; Moving\rMoving --\u0026gt; Still\rMoving --\u0026gt; Crash\rCrash --\u0026gt; [*]\r Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example\r- [x] Write diagram example\r- [ ] Do something else\r renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header |\r| ------------- | ------------- |\r| Content Cell | Content Cell |\r| Content Cell | Content Cell |\r renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}}\rA Markdown aside is useful for displaying notices, hints, or definitions to your readers.\r{{% /alert %}}\r renders as\n\rA Markdown aside is useful for displaying notices, hints, or definitions to your readers.\r\r\rIcons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R\r renders as\n\r Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"e37aa74e3de7182302ddabe4a168a315","permalink":"https://vipanchikatthula.github.io/reference/posts/technical_writing_post/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/reference/posts/technical_writing_post/","section":"Reference","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"Reference"}]