<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | Vipanchi Reddy</title>
    <link>https://vipanchikatthula.github.io/tags/nlp/</link>
      <atom:link href="https://vipanchikatthula.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 08 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>NLP</title>
      <link>https://vipanchikatthula.github.io/tags/nlp/</link>
    </image>
    
    <item>
      <title>Document Similarity using POS Tags</title>
      <link>https://vipanchikatthula.github.io/project/document-similarity-with-postags/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://vipanchikatthula.github.io/project/document-similarity-with-postags/</guid>
      <description>&lt;p&gt;Finding Similarity between documents using Jaccard Similarity and including POS Tags of the words in the model to better find the synchronous relationship between documents.&lt;/p&gt;
&lt;p&gt;Add anything you want here&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>400K Tweet analysis on PySpark</title>
      <link>https://vipanchikatthula.github.io/project/pyspark-operations-on-amazons-tweets/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://vipanchikatthula.github.io/project/pyspark-operations-on-amazons-tweets/</guid>
      <description>&lt;h1 id=&#34;pyspark-operations-on-amazon-tweets&#34;&gt;PySpark Operations On Amazon Tweets&lt;/h1&gt;
&lt;p&gt;This repository shows the implementation of Spark context, Spark SQL context on Amazon Tweets data set with 400k Tweets. I dealt with tweet_id (id_str), Tweet_created_time, Retweet_count, Favourite_count to find the days with high influx of tweets.&lt;br&gt;
Analyzed the tweets on the busiest day to find the words that were repeated the most in the selected tweets.&lt;/p&gt;
&lt;p&gt;Summary of operations:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Import csv file:&lt;/strong&gt;&lt;br&gt;
data = spark.read.format(&amp;ldquo;csv&amp;rdquo;).option(&amp;ldquo;header&amp;rdquo;,&amp;ldquo;true&amp;rdquo;).load(&amp;ldquo;filepath/filename.csv&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Selecting the required columns from a data frame:&lt;/strong&gt;&lt;br&gt;
data.select(&amp;ldquo;column_name1&amp;rdquo;,&amp;lsquo;colname2&amp;rsquo;,&amp;lsquo;colname3&amp;rsquo;,&amp;lsquo;colname4&amp;rsquo;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Printing data types of columns:&lt;/strong&gt;&lt;br&gt;
types = [f.dataType for f in data1.schema.fields]&lt;br&gt;
types&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Printing distinct values of a column:&lt;/strong&gt;&lt;br&gt;
data.select(&amp;ldquo;column_name&amp;rdquo;).distinct().show()&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Parsing a column to create additional columns:&lt;/strong&gt;&lt;br&gt;
from pyspark.sql.functions import split&lt;br&gt;
tweet_created_at is in the format: &amp;ldquo;Tue Nov 01 02:39:55 +0000 2016&amp;rdquo;&lt;br&gt;
a = split(dat_filtered[&amp;ldquo;tweet_created_at&amp;rdquo;], &#39; &amp;lsquo;)&lt;br&gt;
dat_filtered = dat_filtered.withColumn(&amp;lsquo;Month&amp;rsquo;, a.getItem(1))&lt;br&gt;
dat_filtered = dat_filtered1.withColumn(&amp;lsquo;Date&amp;rsquo;, a.getItem(2))&lt;br&gt;
dat_filtered = dat_filtered1.withColumn(&amp;lsquo;Year&amp;rsquo;, a.getItem(5))&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. Concatenating multiple columns to form a new one&lt;/strong&gt;&lt;br&gt;
dat_filtered.select(concat(col(&amp;ldquo;Month&amp;rdquo;), lit(&amp;rdquo; &amp;ldquo;), col(&amp;ldquo;Date&amp;rdquo;),lit(&amp;rdquo; &amp;ldquo;), col(&amp;ldquo;Year&amp;rdquo;)).alias(&amp;ldquo;Date&amp;rdquo;))&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7. Importing SQL funtions col,lit&lt;/strong&gt;&lt;br&gt;
import pyspark.sql.functions as sq 
dat_filtered.withColumn(&amp;ldquo;tweet_created_at&amp;rdquo;,sq.concat(col(&amp;ldquo;Month&amp;rdquo;), sq.lit(&amp;rdquo; &amp;ldquo;), sq.col(&amp;ldquo;Date&amp;rdquo;),sq.lit(&amp;rdquo; &amp;ldquo;), sq.col(&amp;ldquo;Year&amp;rdquo;)))&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8. Aggregations on dataframe&lt;/strong&gt;&lt;br&gt;
df.groupby(df.date).agg(sq.count(&amp;lsquo;id_str&amp;rsquo;).alias(&amp;ldquo;count_of_tweets&amp;rdquo;))&lt;br&gt;
Above command counts the number of tweets grouped by the date&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9. Initializing spark context and sql context to perform SQL queries&lt;/strong&gt;&lt;br&gt;
conf = pyspark.SparkConf()&lt;br&gt;
sc = pyspark.SparkContext.getOrCreate(conf=conf)&lt;br&gt;
from pyspark.sql import SQLContext&lt;br&gt;
sqlcontext = SQLContext(sc)&lt;br&gt;
counts.registerTempTable(&amp;ldquo;tmpcounts&amp;rdquo;)&lt;br&gt;
counts_ordered = sqlcontext.sql(&amp;ldquo;SELECT * FROM tmpcounts order by count_of_tweets desc limit 5&amp;rdquo;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis on Tweets</title>
      <link>https://vipanchikatthula.github.io/project/twitter-sentiment-analysis/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://vipanchikatthula.github.io/project/twitter-sentiment-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Jaccard Cosine Similarity</title>
      <link>https://vipanchikatthula.github.io/project/jaccard-cosine-similarity/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://vipanchikatthula.github.io/project/jaccard-cosine-similarity/</guid>
      <description>&lt;h1 id=&#34;text-analytics---jaccard-and-cosine-similarity&#34;&gt;Text Analytics - Jaccard and Cosine Similarity&lt;/h1&gt;
&lt;p&gt;Repository to showcase projects related to text analytics and Natural Language Processing (NLP)&lt;/p&gt;
&lt;p&gt;Jaccard Similarity:
The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations.&lt;/p&gt;
&lt;p&gt;#How to calculate:
The formula to find the Index is:&lt;/p&gt;
&lt;p&gt;Jaccard Index = (the number in both sets) / (the number in either set) * 100&lt;/p&gt;
&lt;p&gt;The same formula in notation is:
J(X,Y) = |X∩Y| / |X∪Y|
In Steps, that’s:&lt;/p&gt;
&lt;p&gt;1.Count the number of members which are shared between both sets.
2.Count the total number of members in both sets (shared and un-shared).
3.Divide the number of shared members (1) by the total number of members (2).
4.Multiply the number you found in (3) by 100.&lt;/p&gt;
&lt;p&gt;Cosine Similarity:
Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. Any document can be represented by thousands of attributes, each recording the frequency of a particular word (such as a keyword) or phrase in the document. Thus, each document is an object represented by what is called a term-frequency vector.&lt;/p&gt;
&lt;p&gt;I used python function to calculate the text similarity rather than using the traditional way of calculating by using the formula.&lt;/p&gt;
&lt;p&gt;Sources:
&lt;a href=&#34;https://www.statisticshowto.datasciencecentral.com/jaccard-index/&#34;&gt;https://www.statisticshowto.datasciencecentral.com/jaccard-index/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/topics/computer-science/cosine-similarity&#34;&gt;https://www.sciencedirect.com/topics/computer-science/cosine-similarity&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
